{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "313acaed",
   "metadata": {},
   "source": [
    "# Vizugy portal scraper\n",
    "\n",
    "We start by scraping the latest available water level data from the website https://www.vizugy.hu/?mapData=VizmerceLista#mapData using the BeautifulSoup (bs4) library. This website provides information on water level measuring stations across Hungary. We extract the necessary data using the appropriate HTML selectors to ensure accuracy and reliability.\n",
    "\n",
    "Once we have the latest available data, we proceed to iterate through each link on the page to access the historical water level data for each measuring station (hourly data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8f8a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48387302",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIZUGY_WEBPAGE = 'https://www.vizugy.hu/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7deac9",
   "metadata": {},
   "source": [
    "Get the first table from the `VizmerceLista` site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca98a030",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_html(f'{VIZUGY_WEBPAGE}?mapData=VizmerceLista#mapData', extract_links=\"body\")\n",
    "df = df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3838c89",
   "metadata": {},
   "source": [
    "All columns here are tuple typed. First is the value of the cell, second is the link (if it is a link)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630cc4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8776ef17",
   "metadata": {},
   "source": [
    "We simply split the tupe to `_val` and `_url` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592632f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    df[[f'{col}_val', f'{col}_url']] = pd.DataFrame(df[col].to_list(), index=df.index)\n",
    "    df.drop(col, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee5dff",
   "metadata": {},
   "source": [
    "This is how the link look like for a subpage (Station page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc06e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0][\"Vízmérce_url\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f011217",
   "metadata": {},
   "source": [
    "Let's got through all subpage (station page) and collect the hourly table. All of this data will be available as `hourly_data`. Scraping these hundreds page took a while (5 mins or so)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d05848",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    df2 = pd.read_html(f'{VIZUGY_WEBPAGE}{df.iloc[index][\"Vízmérce_url\"]}',parse_dates=True)\n",
    "    df2[1][\"Vízmérce\"] = df.iloc[index][\"Vízmérce_val\"]\n",
    "    df_list.append(df2[1])\n",
    "\n",
    "hourly_data = pd.concat(df_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416b6e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
